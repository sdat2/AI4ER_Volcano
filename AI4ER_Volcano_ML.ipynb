{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4ER Environmental Risk Practical Session: Image Analysis of Volcanic Rocks\n",
    "- Friday 6th November, 2020 - online session - postponed to 13th.\n",
    "- Lecturer: John Maclennan\n",
    "- Practical Experts: Matt Ball and Norbert Toth.\n",
    "\n",
    "## Purpose of this session\n",
    "### Broader Context\n",
    "Image analysis is a field with clear targets for machine learning approaches. Furthermore, many environmental applications use spectral imaging - the acquisition of a spectrum at each pixel. This is a language that will likely become familiar to you from the treatment of satellite data to map features on the global scale. At the other extreme of spatial scales, the examination of environmental materials by electron microscopy is evolving rapidly due to rapid advances in instrumentation the application of techniques developed in materials science to earth and environmental problems. \n",
    "### Key Skills\n",
    "- Run Jupyter Notebook in Jupyter Lab from Binder\n",
    "- Use of HyperSpy package for handling electron microscope data\n",
    "- Use of Skimage package for image processing\n",
    "- Application of Machine Learning techniques to Phase Identification\n",
    "- Masking and Segmentation of phases\n",
    "- Extraction of rock textural properties (Crystal Sizes and Shapes)\n",
    "- Estimating magmatic timescales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a id='link1'></a>\n",
    "\n",
    "<a href='#link10'> **1.0 What is a Phase Map of a Volcanic Rock?**</a>\n",
    "\n",
    "<a href='#link20'> **2.0 Using Machine Learning to guide phase mapping**</a>\n",
    "\n",
    "<a href='#link21'> **2.1 Load in Packages - Numpy, Scipy, Skimage, Hyperspy**</a>\n",
    "\n",
    "<a href='#link22'> **2.2 Load and Plot EDS Data**</a>\n",
    "\n",
    "<a href='#link23'> **2.3 Compositional information in Spectra**</a>\n",
    "\n",
    "<a href='#link30'> **3.0 Machine Learning for Separating Phases**</a>\n",
    "\n",
    "<a href='#link31'> **3.1 Identifying the number of phases**</a>\n",
    "\n",
    "<a href='#link32'> **3.2 Defining the number of principal components**</a>\n",
    "\n",
    "<a href='#link40'> **4.0 Mask extraction**</a>\n",
    "\n",
    "<a href='#link50'> **5.0 Plotting a Phase Map**</a>\n",
    "\n",
    "<a href='#link60'> **6.0 Extracting Textural Information from the Maps**</a>\n",
    "\n",
    "<a href='#link61'> **6.1 Modal Analysis**</a>\n",
    "\n",
    "<a href='#link62'> **6.2 Segmentation of Image into Crystals**</a>\n",
    "\n",
    "<a href='#link62'> **6.3 Crystal Size Distributions**</a>\n",
    "\n",
    "<a href='#link64'> **6.4 Aspect Ratios**</a>\n",
    "\n",
    "\n",
    "\n",
    "This notebook contains a commented workflow on running machine learning on EDS data for phase separation and quantifying morphology.\n",
    "\n",
    "Original written by Matt Ball, updated by John Maclennan and with new code from Norbert Toth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 What is a Phase Map of a Volcanic Rock? <a id='link10'></a>\n",
    "\n",
    "Here is an example of a [phase map](https://academic.oup.com/view-large/figure/82008133/egu058f10p.png) of a rock from the active volcanic zones of Iceland.\n",
    "The image is from [Neave et al., 2014](https://academic.oup.com/petrology/article/55/12/2311/1558945).\n",
    "It was generated using a commerical system called QEMSCAN that is a piece of software that is linking to a scanning electron microcope (SEM).\n",
    "You can see from the scale that we are looking at map of a 'thin section' prepared from the rock - you saw some of these in the lecture slides, as well as the SEM in the Department of Earth Sciences. The rocks are made of 'phases'. Each phase has a distinctive composition and set of thermodynamic properties which are determined by the crystalline (or glass) structure of the cooled lava. These phase names (e.g. olivine, plagioclase, clinopyroxene) are displayed on the legend of the figure along with the colours. In essence the QEMSCAN system is performing an advanced form of colour-by-numbers. The question is - how does it do this?\n",
    "\n",
    "This raw data that goes into generation of these maps is acquired using a technique known as Energy Dispersive Spectroscopy (EDS). The SEM fires an electron beam at a focussed point on the sample surface, causing electrons to shift around energy levels and then to release X-rays - which are gathered by the EDS detectors. A spectrum is generated and the position of peaks can be attributed to characteristic energies that are associated with elements. The peak height is linked to the concentration of the element in the sample.\n",
    "\n",
    "The QEMSCAN software converts the spectrum for each pixel into an estimate of the chemical composition of that point, and then uses a series of logical filters on the compositional data to match each pixel to a phase. For example, olivine is a mineral that contains This means that a large database of phases has to be accessed and the selection of possible phases is not guided by the compositional variations observed within the sample itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Using Machine Learning to guide phase mapping: Setup <a id='link20'></a>\n",
    "\n",
    "An alternative approach with considerable promise in this field is to base the identification of phases directly on the available EDS data. In order to understand how we might be able to use this approach we are going to work through a set of activties in this notebook, starting from data input/output through to quantification of the rock textures.\n",
    "\n",
    "The datasets are large and some of the ML processing is computationally expensive. For this practical we will therefore work with relatively small portions of the images, but will import results from larger images that have been calculated beforehand - Matt or Norbert might well say - \"Here's one that I made earlier\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load in Packages - Numpy, Scipy, Skimage, Hyperspy <a id='link21'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T12:14:21.841940Z",
     "start_time": "2020-11-13T12:13:31.400713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/anaconda3/envs/volctest/lib/python3.8/site-packages/pyUSID/viz/__init__.py:16: FutureWarning: Please use sidpy.viz.plot_utils instead of pyUSID.viz.plot_utils. pyUSID.plot_utils will be removed in a future release of pyUSID\n",
      "  warn('Please use sidpy.viz.plot_utils instead of pyUSID.viz.plot_utils. '\n",
      "WARNING:hyperspy_gui_traitsui:The module://ipykernel.pylab.backend_inline matplotlib backend is not compatible with the traitsui GUI elements. For more information, read http://hyperspy.readthedocs.io/en/stable/user_guide/getting_started.html#possible-warnings-when-importing-hyperspy.\n",
      "WARNING:hyperspy_gui_traitsui:The traitsui GUI elements are not available.\n"
     ]
    }
   ],
   "source": [
    "import hyperspy.api as hs\n",
    "hs.preferences.GUIs.enable_traitsui_gui = False\n",
    "hs.preferences.save()\n",
    "import numpy as np\n",
    "from numpy import mean, std, median, sqrt, exp, log, delete\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "\n",
    "import scipy\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.stats import sem, t, norm, gaussian_kde, mstats, iqr\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import skimage as ski\n",
    "from skimage.segmentation import random_walker\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import exposure\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "from skimage import restoration\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load and Plot EDS Data <a id='link22'></a>\n",
    "\n",
    "Use Hyperspy to load in the EDS data in its bespoke .bcf format as output by the software that runs the detectors. You can look in the Hyperspy documentation to figure out what some of these commands are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:01:22.113198Z",
     "start_time": "2020-11-13T13:01:17.573608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Signal2D, title: Ch 0, dimensions: (|1820, 1295)>,\n",
       " <Signal2D, title: Ch 0, dimensions: (|2000, 2000)>,\n",
       " <EDSSEMSpectrum, title: EDX, dimensions: (1820, 1295|2048)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a = hs.load('your_filename.bcf')\n",
    "# a = hs.load('JM1_OL7_map1.bcf')\n",
    "a = hs.load('JM1_OL7_EDS.bcf')\n",
    "#https://drive.google.com/file/d/1yHQXNsE8Pls8lfTuLQTI9R46uqpY-Q0s/view?usp=sharing \n",
    "#gdrive_file_id = '1yHQXNsE8Pls8lfTuLQTI9R46uqpY-Q0s'\n",
    "#a = hs.load(f'https://docs.google.com/uc?id={gdrive_file_id}&export=download')\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:01:40.532778Z",
     "start_time": "2020-11-13T13:01:40.520063Z"
    }
   },
   "outputs": [],
   "source": [
    "s = a[2]\n",
    "s.axes_manager.set_signal_dimension(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hs.preferences.gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:02:07.764769Z",
     "start_time": "2020-11-13T13:01:53.654734Z"
    }
   },
   "outputs": [],
   "source": [
    "s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two new plot windows should have appeared. They does not look very impressive. One plot is a map and you can see the scale bar.\n",
    "The bottom plot shows a spectrum, with a very small number of points. Can you figure out from the [Hyperspy manual](http://hyperspy.org/hyperspy-doc/current/user_guide/visualisation.html#multidimensional-spectral-data) what you have plotted here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you zoom in to the origin (look carefully at all the information on the plots) then you should be able to see a red line round the selected pixel. You can move this point (as long as pan and zoom are not selected) and examine the spectra of other individual pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:02:33.288293Z",
     "start_time": "2020-11-13T13:02:33.282902Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to play around with the Region of Interest commands in Hyperspy plotting. This should start to give you an even clearer feeling for what the low-quality spectrum - low number of counts - available at each point looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:02:49.121106Z",
     "start_time": "2020-11-13T13:02:39.042959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using interactive roi\n"
     ]
    }
   ],
   "source": [
    "roi = hs.roi.RectangularROI(left=120, right=460., top=300, bottom=560)\n",
    "s.plot()\n",
    "sc = roi.interactive(s)\n",
    "sc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:03:03.336783Z",
     "start_time": "2020-11-13T13:03:03.330568Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Compositional information in Spectra <a id='link23'></a>\n",
    "\n",
    "Hyperspy contains a database of [elemental properties](http://hyperspy.org/hyperspy-doc/current/user_guide/eds.html#elemental-database). Add a code cell in below to print where you would expect to find peaks for Mg and Fe in the spectra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to add labels to peaks found on the plotted spectra, as shown below. You might like to compare these with a high quality spectrum (lots of counts) for the mineral phase called olivine (e.g. in the [Mindat database](https://www.mindatnh.org/Forsterite%20Gallery.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T13:04:17.535763Z",
     "start_time": "2020-11-13T13:03:12.839498Z"
    }
   },
   "outputs": [],
   "source": [
    "s.add_elements(['Si','Mg','Fe','Al','Ca']) # it is also possible to access detected elements from the bcf file from s.metadata.Sample.elements\n",
    "s.plot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Subset the loaded data <a id='link24'></a>\n",
    "\n",
    "The data file provided is very large and too big for us to process in the Binder environment (2 Gb memory) or on typical desktops or laptops from 2019/20. It is therefore necessary to subset the data before carrying out computationally demanding work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_mini = s.data[0:200,500:800,:]\n",
    "s_min = hs.signals.Signal1D(s_mini)\n",
    "s_min.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_min.set_signal_type(\"EDS_SEM\")\n",
    "s_min.axes_manager[-1].name = 'E'\n",
    "s_min.axes_manager['E'].units = 'keV'\n",
    "s_min.axes_manager['E'].scale = 0.01\n",
    "#Set the offset, the difference between the measured peak location and the true location, often an\n",
    "#artificial zero peak is added to EDS data which can be used for this. This may need to be done a few\n",
    "#times to correctly set this value\n",
    "s_min.axes_manager['E'].offset = -0.0\n",
    "#Set the beam energy to the beam energy the EDS data was collected at in keV\n",
    "s_min.metadata.Acquisition_instrument.SEM.beam_energy = 20\n",
    "s_min.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading in Hyperspy Format\n",
    "\n",
    "When files are saved in Hyperspy format they can be rapidly reloaded as required. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Overwrite 'your_filename.hspy' (y/n)?\n",
      " y\n"
     ]
    }
   ],
   "source": [
    "s.save('your_filename') # This is saving the full file (not the subset area) in hspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hs.load('your_filename.hspy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Machine Learning for Separating Phases <a id='link30'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identifying the number of phases <a id='link31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we run a PCA (Principal Component Analysis) on the data to identify the number of significant principal components. The different phases will consist of both a loading map and corresponding spectrum. These can be cycled through using the arrow keys. Also plotted are two scree plots, which show the percentage of the data which can be explained by each principal component, one of which is plotted with a logarithmic y-axis, due to the large amount of data, this will be too slow to perform on the full dataset, so we will instead use the subset of data we created earlier, whilst the results of the machine learning on the full set can be loaded in and viewed.\n",
    "The cell below produces a number of plots in windows - make sure that you can see them all and interact with them - using the slider window in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=SVD\n",
      "  output_dimension=20\n",
      "  centre=None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7366b995a74ced9fcfb7e934cfcc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_min.change_dtype('float32')\n",
    "s_min.decomposition(True, algorithm='SVD', output_dimension=20)\n",
    "s_min.plot_decomposition_results()\n",
    "s_min.plot_explained_variance_ratio(log=False)\n",
    "s_min.plot_explained_variance_ratio(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the results of this PCA contain both negative loading intensities as well as negative spectra, neither of which is physically meaningful. This first step is required to identify the number of significant phases which later machine learning can pull apart. How many phases do you think are being picked up by this PCA approach? How can we tell whether the recovered principal components are likely to correspond to natural features of the rock sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can save the results of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.learning_results.save('your_filename_PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one I made earlier! Now we can load in the PCA results on the whole dataset to compare to our subsection. Alternatively we can load in results of any previous PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.learning_results.load('AI4ER_JM1_OL7_PCA.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af51ed4902c4fa69dbfb2bac687125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'EDX\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.plot_decomposition_results()\n",
    "s.plot_explained_variance_ratio(log=False)\n",
    "s.plot_explained_variance_ratio(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, how many of the PCs do you think might be picking up aspects of the rock sample - rather than analytical noise or artifacts? The answer here might be different to the previous case - because here we are looking at the results from the full area that was scanned, not a subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Defining the number of principal components <a id='link32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components can be defined either by the number which is below a critical value of variance, by the shape of the scree plot, by the form of the associated spectrum or by the spatial pattern defined in the images. Pick a number - perhaps discuss the best strategy with a demonstrator if you are unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC = 'number of principal components' # Replace string with number\n",
    "PC = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use this number of principal components to run a non-negative matrix factorisation, a clustering algorithm which produces only positive loadings and spectra, removing the issue of non physically meaningful results. This makes use of machine learning algorithms through scikit-learn. Again we will perform this only on the subset of the data and load in a previously collected result for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=NMF\n",
      "  output_dimension=3\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf68fc9a7c2458392ed016138cc61e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_min.decomposition(True, algorithm='NMF', output_dimension=PC)\n",
    "s_min.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be starting to look quite good - use the slider window to examine the maps and spectra associated with the factors obtained.  Hopefully you can see crystal shapes on the maps and note that the spectra look physically plausible - in that at least they all have positive count values. Another remarkable feature is how smooth and sharp the spectra associated with each factor are - certainly in comparison to the noisy low-count spectra from each individual point. This is the power of looked for correlated signals across thousands of pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can save the results of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.learning_results.save('your_filename_NMF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again here's another pre-NMF-ed set of results. We could alternatively load in other previously collected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.learning_results.load('AI4ER_JM1_OL7_NMF.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot these results up and attempt to label the spectra for the position of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0960808758134362acde0df3b0c837fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Decomposition component index', layout=Layout(width='15%')), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.plot_decomposition_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plot, you can move between the spectra of the factors by clicking on the red line in the slider and moving it vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "snspec = s.get_decomposition_factors()\n",
    "snspec.add_elements(['Si','Mg','Fe','Al','Ca','Cr'])\n",
    "snspec.plot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which phases do the different factors correspond to? Use the [Mindat](https://www.mindat.org/) database to check on mineral compositions.\n",
    "In this type of rock, which is called a basalt, the common mineral phases are olivine, clinopyroxene, plagioclase, magnetite and a Cr-bearing spinel. **Be careful - the peaks at less than about 1 keV show overlap here which makes it difficult for the peak picking alhorithm to always correctly label a peak with the correct element. Also - one of the tallest peaks may be labelled with the element right at the top of the plot.**\n",
    "\n",
    "The basaltic liquid also quenches to a glass when it cools rapidly upon eruption. \n",
    "You can find some likely glass compositions (as well as the answers to the questions above) in [Larsen and Pedersen, 2000](https://academic.oup.com/petrology/article/41/7/1071/1457495#99082445) - glass compositions in Table 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Mask extraction <a id='link40'></a>\n",
    "\n",
    "The above NMF factors are not yet in their most useful form - they need to be converted to 'binary images': images of only 1s and 0s. With such 'masks' we can start to extract information about the seprated phases. This can be done manually by setting a suitable threshold below which all pixels are 0 and 1 above or automatically by using a suitable algorithm - eg. [Otsu's method here](https://en.wikipedia.org/wiki/Otsu%27s_method) or [here](https://scikit-image.org/docs/0.7.0/api/skimage.filter.thresholding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_images = []\n",
    "phases = [0,1,2,3]\n",
    "components = s.get_decomposition_loadings()\n",
    "\n",
    "for i in phases:\n",
    "    #.compute() is required below due to lazy loading of data\n",
    "    image = components.inav[i].data.copy()\n",
    "    image = image/image.max() #normalize loading values\n",
    "    \n",
    "    phase_images.append(image)\n",
    "    \n",
    "    #creating bins from 0.01 to 1 with width 0.01.\n",
    "    bins = [0.01*(n+1) for n in range(99)]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(image.flatten(), bins)\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you understand what these four plots mean. How might you use them to create masks? What are the likely causes of spread on the histograms? It may be helpful to go back and look at the maps of the loadings from the end of the last section.\n",
    "\n",
    "The cell below shows some manual choices of value for the binary thresholding. You might want to test what happens if you use different values or explore the Otsu method referred to above for these choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = [0.26,0.3,0.36,0.18]\n",
    "post_thresh = []\n",
    "\n",
    "for n in range(len(phase_images)):\n",
    "    img = phase_images[n].copy()\n",
    "    \n",
    "    img[img<thresh[n]] = 0\n",
    "    img[img>thresh[n]] = 1\n",
    "    \n",
    "    post_thresh.append(img)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is starting to look promising, you might also be able to spot some artifacts on these images. Can you describe these? How do you think that you might deal with these artifacts?\n",
    "\n",
    "Make sure that you have a clear idea in your mind about what the identity of each phase is (e.g. glass, plagioclase, olivine). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible piece of further processing of the images to handle noise in the data can be carried out using 'morphological transformations' (https://scikit-image.org/docs/dev/api/skimage.morphology.html) to get rid of noise present due to having phases of similar compositions - one of the limitations of using EDS only. 'Area_opening' operation gets rid of objects lower than a given size and so it is used to remove single pixels or very small clusters of pixels which can be attributed to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = 10 #adjust this value or make into an array to give each mask an individual value\n",
    "phase_mask = []\n",
    "\n",
    "for n in range(len(post_thresh)):\n",
    "    #if min_size is an array use min_size[n] below\n",
    "    mask = morphology.area_opening(post_thresh[n], min_size)\n",
    "    mask = morphology.area_closing(mask, min_size)\n",
    "    phase_mask.append(mask)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any good ideas about how to eliminate other artifacts in these results, please do go ahead and writed some cells in your notebook to explore your thinking. For example, if you go all theway back to the original PCA results for the whole region, you might see a potential source of problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Plotting a Phase Map <a id='link50'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to combine these images into a single 'phase map'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a23f46130>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create blank image - all zeroes\n",
    "phase_map = np.zeros((len(phase_mask[0]), len(phase_mask[0][0])))\n",
    "abundance = []\n",
    "\n",
    "for i in range(len(phase_mask)):\n",
    "    coords = np.nonzero(phase_mask[i])\n",
    "    abundance.append(len(coords[0]))\n",
    "    for n in range(len(coords[0])):\n",
    "        phase_map[coords[0][n]][coords[1][n]] = i+1\n",
    "\n",
    "#should show the phase map in grayscale\n",
    "plt.imshow(phase_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also produce a fancy version of this image, labelled up with your identified phases by editing the 'phase_names' line in the cell and your own colour scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with RGB codes for the desired colours\n",
    "# index 0 is unclassified and the rest have to be in order of the phases in the above images from loadings\n",
    "colour = ['black', 'palegoldenrod', 'olivedrab', 'darkcyan','purple' ]\n",
    "cMap = mpl.colors.ListedColormap(colour)\n",
    "\n",
    "#create plot\n",
    "fig, ax = plt.subplots(figsize = (9,5))\n",
    "plot = plt.imshow(phase_map, cmap=cMap, interpolation = None)\n",
    "\n",
    "#get values present in above image\n",
    "values = np.unique(phase_map.ravel())\n",
    "\n",
    "# get the colors of the values, according to the colormap used by imshow\n",
    "colors = [ plot.cmap(plot.norm(value)) for value in values]\n",
    "\n",
    "# Name each colour and create a patch (proxy artist) for every color \n",
    "phase_names = [\"Unclassified\", \"Phase1\", \"Phase2\", \"Phase3\", \"Phase4\"]\n",
    "patches = [ mpl.patches.Patch(color=colors[i], label=phase_names[i] ) for i in range(len(values)) ]\n",
    "\n",
    "# put those patched as legend-handles into the legend\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "#set tick marks here, including how many to show\n",
    "x_ticks = np.linspace(0,len(phase_map[0]),7)\n",
    "y_ticks = np.linspace(0,len(phase_map),5)\n",
    "\n",
    "# What those pixel locations correspond to in data coordinates.\n",
    "# Also set the float format here\n",
    "x_ticklabels = [\"{:6.2f}\".format(i) for i in (x_ticks*s.axes_manager[0].scale)/1000]\n",
    "y_ticklabels = [\"{:6.2f}\".format(i) for i in (y_ticks*s.axes_manager[0].scale)/1000]\n",
    "\n",
    "#apply the above changes to the plot\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticklabels)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticklabels)\n",
    "\n",
    "plt.title(\"Labelled phase map - axis values are shown in mm.\", fontsize = 11)\n",
    "plt.show()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with the plot, you can save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save above plot:\n",
    "fig.savefig(\"Example_labelled_map.png\", dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Extracting Textural Information from the Maps <a id='link60'></a>\n",
    "\n",
    "Rock textures can be used to understand the physical processes that occurred in a magmatic system in association with eruption.\n",
    "\n",
    "### 6.1 Modal Analysis <a id='link61'></a>\n",
    "\n",
    "This simple chacacterisation is used to describe the volumetric proportions of the phases in a sample.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modal Phase1 proportion: 0.5982155830081246\n",
      "Modal Phase2 proportion: 0.3547650548965913\n",
      "Modal Phase3 proportion: 0.04522879135192792\n",
      "Modal Phase4 proportion: 0.001790570743356085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Modal phase proportions')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = phase_names[1:].copy()\n",
    "\n",
    "#measure phase proportions by area here\n",
    "total_pix = sum(abundance)\n",
    "\n",
    "phase_proportions = []\n",
    "for i in range(len(abundance)):\n",
    "    phase_proportions.append(abundance[i]/total_pix)\n",
    "    print(\"Modal \" + names[i] + \" proportion: \" + str(phase_proportions[i]))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = range(len(abundance))\n",
    "ax.bar(x, phase_proportions,tick_label = names)\n",
    "ax.set_ylabel(\"Modal abundance by area\")\n",
    "fig.suptitle(\"Modal phase proportions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Segmentation of Image into Crystals <a id='link62'></a>\n",
    "\n",
    "Using standard Data Analysis libraries - suck as skimage, it is possible to rapidly quantify crystal sizes and shapes from binary phase masks generated above. See the 'measure' module within skimage for more info - https://scikit-image.org/docs/dev/api/skimage.measure.html.\n",
    "\n",
    "In this example the Plagioclase phase mask will be used to show how this may be done. The first step is to actually determine where the different crystals are within the phase mask and label them. A simple approach is to label by connectivity of pixels, however it assumes separate crystals are not touching and clusters can lead to significant errors. An image processing approach is to use marker based watershed segmentation to separate clusters; it can work well, but requires some manual tweaking. The algorithm is based on segmentation of touching circles - it doesn't perform as well with non circular objects. The smartest approach may be to use EBSD data to segment clusters based on contrast from crystallographic orientation.\n",
    "\n",
    "Experiment with the area_closing function to see how effectively it fills in the gaps in the crystals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a25885490>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filled objects are best to perform watershed as inclusions are treated as 'background' in binary mask so it may\n",
    "#be worth filling in the crystals using morphological transformations to improve segmentation quality\n",
    "\n",
    "plag_mask = phase_mask[2].copy()\n",
    "plag_mask = morphology.area_closing(plag_mask, 64)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,24))\n",
    "ax.imshow(plag_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, perform the segmentation with the watershed algorithm. Again, try to experiment with to see how effectively you can segment the objects. Each object is given a different colour here - you will be able to spot some large crystals that have been incorrectly segmented, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a2670bc70>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_mask = plag_mask.copy()\n",
    "\n",
    "distance = ndi.distance_transform_edt(ws_mask)\n",
    "\n",
    "#tune footprint to get wanted/optimum segmentation of touching objects?\n",
    "#large objects may get oversegmented whilst smaller ones won't segment at all - it's impossible to get it perfect\n",
    "local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((25,25)), labels=ws_mask)\n",
    "\n",
    "markers = ski.morphology.label(local_maxi, connectivity = 2)\n",
    "labels_ws = ski.segmentation.watershed(-distance, markers, mask=ws_mask)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,24))\n",
    "ax.imshow(labels_ws, cmap = 'twilight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Crystal Size Distributions <a id='link63'></a>\n",
    "\n",
    "The plagioclase crystals tend to be elongate and it is therefore useful to define the size (a length) as the square root of the area as counted from the number of pixels in each segmented object. We can then start by plotting a fequency-size histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get spatial resolution of individual pixels; the 0th axis is a spatial dimension so can be used here\n",
    "#if it wasn't clear it's a spatial dimension it can be modified to: \n",
    "scale = s.axes_manager.navigation_axes[0].scale\n",
    "#scale = s.axes_manager[0].scale\n",
    "\n",
    "\n",
    "plag = labels_ws.copy()\n",
    "properties = measure.regionprops(labels_ws)\n",
    "ROI_area = scale**2 * len(plag)*len(plag[0])\n",
    "\n",
    "#in the approximate CSD size is measure in Area^{0.5}\n",
    "size = [np.sqrt(cryst.area)*scale for cryst in properties]\n",
    "\n",
    "#creating bins from 0.01 to 1 with width 0.01.\n",
    "bins = [1.0*(n+1) for n in range(99)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(size, bins)\n",
    "ax.set_xlabel(\"Area^{0.5}\")\n",
    "ax.set_ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try now to consider how the limitations of the data collection and the processing steps used in this practical might influence the appearance of this histogram. In other words, is this likely to be a good approximation to the natural underlying distribution of crystal sizes in the rock sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent paper by (Neave et al., 2018)[https://www.degruyter.com/view/journals/ammin/102/10/article-p2007.xml] the current favoured method for characterisinhg crystal size distributions is developed, which effectively provides the gradient on the slope of a cumulative distribution of size (normalised to unit area). A number of functions are defined below to perform these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_factor(min_size, max_size, n_bins):\n",
    "    \"\"\"\n",
    "    Calculate the geometric factor required to establish bins.\n",
    "    \n",
    "    Input\n",
    "    --------------------------------------\n",
    "    min_size - lower bound of binning process\n",
    "    max_size - upper bound of binning\n",
    "    n_bins - number of bins generated \n",
    "    \n",
    "    Returns\n",
    "    --------------------------------------\n",
    "    factor - the geometric factor required for the binning process\n",
    "    correction - log (base factor) of lower bound of the binning process\n",
    "    \"\"\"\n",
    "    ratio = max_size/min_size\n",
    "    factor = ratio**(1/n_bins)\n",
    "    correction = np.log(min_size)/np.log(factor)\n",
    "    return factor, correction\n",
    "    \n",
    "    \n",
    "def gen_geo_bins(min_size, max_size, n_bins):\n",
    "    \"\"\"\n",
    "    Generate geometric bins required.\n",
    "    \n",
    "    Input\n",
    "    ----------------------------------\n",
    "    min_size - lower bound of binning process\n",
    "    max_size - upper bound of binning\n",
    "    n_bins - number of bins generated   \n",
    "    \n",
    "    Returns\n",
    "    -----------------------------------------------------\n",
    "    bins - lower bound for each bin\n",
    "    bw - bin widths\n",
    "    x_grid - middle values for the bins to be used for plot\n",
    "    \"\"\"\n",
    "    factor, correction = geo_factor(min_size, max_size, n_bins)\n",
    "    \n",
    "    bins = [factor**(i+correction) for i in range(n_bins)]\n",
    "    bw = [(factor**(i+1 + correction) - factor**(i + correction)) for i in range(n_bins)]\n",
    "    x_grid = [(factor**(i+1 + correction) + factor**(i + correction))/2 for i in range(n_bins)]\n",
    "    \n",
    "    return bins, bw, x_grid\n",
    "\n",
    "def gen_csd_plot(size, min_size, max_size, n_bins, area):\n",
    "    \"\"\"\n",
    "    Create approximate CSD plot (Neave et. al. 2017) using geometric binning.\n",
    "    \n",
    "    Inputs\n",
    "    ----------------------------------------------\n",
    "    size - array of crystal sizes\n",
    "    min_size - lower bound of binning process\n",
    "    max_size - upper bound of binning\n",
    "    n_bins - number of bins generated \n",
    "    area - area of ROI considered\n",
    "    \n",
    "    Returns\n",
    "    -----------------------------------------------\n",
    "    fig - matplotlib figure object containing the CSD plot\n",
    "    ax - matplotlib axis object for fig above\n",
    "    \"\"\"\n",
    "    \n",
    "    #run functions to generate bins and all required parameters\n",
    "    factor, correction = geo_factor(min_size, max_size, n_bins)\n",
    "    bins, bw, x_grid = gen_geo_bins(min_size, max_size, n_bins)\n",
    "        \n",
    "    n_cryst = np.zeros(n_bins)\n",
    "\n",
    "    #apply binning to data\n",
    "    for i in range(len(size)):\n",
    "        if size[i] >= max_size:\n",
    "            pass\n",
    "        elif size[i] < min_size:\n",
    "            pass\n",
    "        else:\n",
    "            index = int((np.log(size[i])/np.log(factor))-correction)\n",
    "            n_cryst[index] += 1\n",
    "    \n",
    "    #generate y axis data\n",
    "    n_cryst_div_bw = [(n_cryst[i]/bw[i])/area for i in range(n_bins)]\n",
    "    ln_cryst_div_bw = np.log(n_cryst_div_bw)\n",
    "    \n",
    "    #create matplotlib plot that is then returned\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x_grid, ln_cryst_div_bw, '.', markersize = 11)\n",
    "    ax.set_xlabel(r\"Area$^{0.5}$ ($\\mu$m)\")\n",
    "    ax.set_ylabel(\"ln(N / bw)\")\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates a plot for the plagioclase areas in the original sample. The y-axis is the number of crystals within a certain size bin within in the area, so has units of mm$^{-3}$. You can experiment with the range and the bin-width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-cc3fdeaf8b97>:82: RuntimeWarning: divide by zero encountered in log\n",
      "  ln_cryst_div_bw = np.log(n_cryst_div_bw)\n"
     ]
    }
   ],
   "source": [
    "#the function used to generate plot uses geometric binning\n",
    "#it requires the following input: size, min_size, max_size - minimum and maximum to be used for binning\n",
    "#n_bins - number of bins used\n",
    "#area - area of entire ROI \n",
    "distribution, ax = gen_csd_plot(size, 5,75,12,ROI_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use such plots to estimate the timescales of crystal growth, such as that shown on Figure 4 of [Cashman and Marsh](https://link.springer.com/content/pdf/10.1007/BF00375363.pdf). Try to estimate $G\\tau$ from the slope of your plot (it would be possibel to write a few lines of code to quantify this). You can see in Table 2 of this paper that some values for plagioclase growth rates, $G$ are provided. Use this information to estimate the timescale of crystal growth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Aspect Ratios <a id='link64'></a>\n",
    "\n",
    "The aspect ratio is the length over width of the crystal. We know from crystallisation experiments on magma samples that high values tend to indicate rapid crystal growth and are associated with short timescales. The paper of Holness et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Aspect Ratio')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_ratio = [cryst.major_axis_length/cryst.minor_axis_length for cryst in properties]\n",
    "#can apply correction proposed by Neave et. al. (2017) to aspect ratios\n",
    "#aspect_ratio_corr = [(1.150*item - 0.195) for item in aspect_ratio]\n",
    "\n",
    "plt.hist(aspect_ratio)\n",
    "plt.xlabel(\"Aspect Ratio\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(size, aspect_ratio, '.')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(r\"Area$^{0.5}$ ($\\mu m$)\")\n",
    "ax.set_ylabel(\"Aspect Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to use the calibration of plagioclase aspect ratios by [Holness et al., 2015](https://link.springer.com/content/pdf/10.1007/s00410-014-1076-5.pdf), presented on p1076 of that paper, to try to estimate crystallisation times. \n",
    "\n",
    "Try to compare your estimates of crystallisation times from the CSDs and the aspect ratios. They might be *extremely* different to each other. Try to consider why these estimates might differ. What could you do to improve the image processing to remove that as a cause of inconsistency between the approaches? What other assumptions in the approach might also leas to inconsistencies in timescales of estimates? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Extra: Application to another dataset <a id='link70'></a>\n",
    "\n",
    "In the introduction to this exercise we considered the merits of the QEMSCAN approach. This software is highly effective at mapping large areas of thin sections rapidly and generating phase maps. A .png file generated by filtering the rough compositional map from a QEMSCAN image is provided in the Moodle folder. This has been filtered to only show positive values in regions where the Mg content is above a threshold (i.e. is olivine) and show a strength of green colour defined by the iron content. If you have the time and the inclination, see if you can read this file into HyperSpy and try to extract some quantitative information about its textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
